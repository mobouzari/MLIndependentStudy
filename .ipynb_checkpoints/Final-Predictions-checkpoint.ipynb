{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "import sklearn\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in file and defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18265, 57)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_by_baseline_flag = True\n",
    "add_scale_cols_onehotencoder = True\n",
    "add_missing_01_cols = True\n",
    "\n",
    "raw_data = pandas.read_csv('contactid_08_29_18.csv', encoding=\"utf-8\") \n",
    "\n",
    "raw_data_copy = raw_data\n",
    "\n",
    "#Filling in empty cells\n",
    "\n",
    "raw_data['Min days between calls'].fillna(0, inplace=True)\n",
    "raw_data['Max days between calls'].fillna(0, inplace=True)\n",
    "raw_data['Min days between texts'].fillna(0, inplace=True)\n",
    "raw_data['Max days between texts'].fillna(0, inplace=True)  \n",
    "\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering by baseline_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_by_baseline_flag:\n",
    "    baseline_flag_raw_data = raw_data.loc['close']=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding in columns for 0/1 for texts and calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_missing_01_cols:\n",
    "    calls_raw_data = pandas.get_dummies(raw_data['Total number of calls'], dummy_na=True, prefix='Calls')\n",
    "    texts_raw_data = pandas.get_dummies(raw_data['Total number of texts'], dummy_na=True, prefix='Texts')\n",
    "\n",
    "    text_call_columns_data = pandas.concat([raw_data, calls_raw_data['Calls_nan']],axis=1)\n",
    "    text_call_columns_data = pandas.concat([text_call_columns_data, texts_raw_data['Texts_nan']],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating scale colunn to be replaced with onehotencoder layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_scale_cols_onehotencoder:\n",
    "    # use pandas.concat to join the new columns with dataframe\n",
    "    scale_columns_raw_data = pandas.concat([raw_data,pandas.get_dummies(raw_data['scale'], prefix='scale')],axis=1)\n",
    "\n",
    "    # now drop the original 'scale' column because we wont need it anymore\n",
    "    scale_columns_raw_data.drop(['scale'],axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to consider data in which the \"Close\" column has data\n",
    "# so that is the first thing we filter and we train/test on that new DataFrame\n",
    "\n",
    "dfclean = updated_raw_data.loc[updated_raw_data['close'].notnull()]\n",
    "if filter_by_baseline:\n",
    "    dfclean = dfclean.\n",
    "#dfclean\n",
    "\n",
    "##########\n",
    "# found some counterexamples where baseline_flag being 1, equated to closeness being 60,80....so not sure what to do with this\n",
    "# dfclean_baseline = updated_raw_data.loc[updated_raw_data['baseline_flag']==0]\n",
    "\n",
    "# This will show us (# of rows, # of columns)\n",
    "dfclean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we look over the workable data and see how clean it is\n",
    "\n",
    "num_of_cells = dfclean.shape[0]*dfclean.shape[1]\n",
    "num_of_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at all the columns with NaN values\n",
    "\n",
    "df_null_columns = ((dfclean.isnull().sum()/dfclean.shape[0])*100).round(decimals=2).astype(str) + '%'\n",
    "df_null_columns.to_frame('nulls')\n",
    "#df_null_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also look at all the rows with at least one NaN value\n",
    "\n",
    "df_null_rows = dfclean.isnull().sum(axis=1)\n",
    "# df_null_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The percentage of rows missing at least one value would be\n",
    "\n",
    "(((df_null_rows!=0).sum()/(df_null_rows.shape[0]))*100).round(decimals=2).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And lastly we consider missing cells in relation to all total cells \n",
    "\n",
    "num_of_missing_cells = (dfclean.isnull().sum()).sum()\n",
    "((num_of_missing_cells/num_of_cells)*100).round(decimals=2).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we begin to train/test the data, in hopes of being able to most accurately predict the closeness column for the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We begin by looking at 2 different introductory Models that helps us visualize the data\n",
    "\n",
    "#GradientBoostingClassifier\n",
    "#GradientBoostingRegressor\n",
    "updated_raw_data.loc[updated_raw_data['close'].notnull()]\n",
    "\n",
    "# Before we begin with these Models, we need to format the data in a manner in which we can work with\n",
    "dfcleanX = dfclean.fillna(-1)\n",
    "# dfcleanX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We filter out the specific columns we are interested in\n",
    "# xdata = dfcleanX[['jailprison' ,'Total number of calls',\n",
    "# 'Total duration of calls',\n",
    "# 'Total incoming calls',\n",
    "# 'Total outgoing calls',\n",
    "# 'Total number of missed calls',\n",
    "# 'Total number of incoming calls, excluding missed',\n",
    "# 'Proportion of incoming calls answered',\n",
    "# 'Proportion of calls incoming',\n",
    "# 'Duration of incoming calls',\n",
    "# 'Duration of outgoing calls',\n",
    "# 'Total number of night calls',\n",
    "# 'Any night call',\n",
    "# 'Number of incoming calls at night',\n",
    "# 'Number of outgoing calls at night',\n",
    "# 'Any night incoming call',\n",
    "# 'Any night outgoing call',\n",
    "# 'Min days between calls',\n",
    "# 'Max days between calls',\n",
    "# 'Total number of texts',\n",
    "# 'Any night text',\n",
    "# 'Number of texts at night',\n",
    "# 'Min days between texts',\n",
    "# 'Max days between texts']].values\n",
    "\n",
    "# type(xdata)\n",
    "# # numpy.ndarray\n",
    "# xdata.shape\n",
    "# # (2869, 24)\n",
    "# print(xdata)\n",
    "\n",
    "all_cols = set(dfcleanX.columns.values)\n",
    "all_cols = all_cols.difference(['Contact.ID', 'close', 'baseline_flag', 'subjectid', 'cat', 'empstatus', 'svy_contact'])\n",
    "xdata = dfcleanX[[i for i in all_cols]].values\n",
    "print(xdata)\n",
    "#type(xdata)\n",
    "#xdata.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implement a random 80/20 split of the data to be used for train/test\n",
    "\n",
    "random.seed = 0\n",
    "indices = np.array(range(2869))\n",
    "random.shuffle(indices)\n",
    "num_train = int(len(indices)*0.8)\n",
    "train_indices = indices[:num_train]\n",
    "#print(train_indices.shape)\n",
    "test_indices =indices[num_train+1:]\n",
    "#print(test_indices.shape)\n",
    "\n",
    "# # namimg variables that will be used for training/testing\n",
    "xtrain = xdata[train_indices]\n",
    "xtest = xdata[test_indices]\n",
    "\n",
    "# ravel creates 1d array\n",
    "y = dfclean[['close']].values.ravel()\n",
    "ytrain = y[train_indices]\n",
    "ytest = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "def transform(ytrain):\n",
    "    if ytrain < 20:\n",
    "        return 0\n",
    "    if ytrain < 40:\n",
    "        return 1\n",
    "    if ytrain < 60:\n",
    "        return 2\n",
    "    if ytrain < 80:\n",
    "        return 3\n",
    "    if ytrain < 100:\n",
    "        return 4\n",
    "    return 5\n",
    "\n",
    "\n",
    "ytrainClassifier = [transform(i) for i in ytrain]\n",
    "ytestClassifier = [transform(i) for i in ytest]\n",
    "# ytestClassifier\n",
    "# xtrain.shape,ytrain.shape\n",
    "plt.hist(ytrainClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBclassifier = GradientBoostingClassifier().fit(xtrain,ytrainClassifier)\n",
    "\n",
    "yhatClassifier = GBclassifier.predict(xtrain)\n",
    "yhatClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhatClassifier = GBclassifier.predict(xtest)\n",
    "accuracy_score(ytestClassifier, yhatClassifier)\n",
    "plt.hist(yhatClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = []\n",
    "train_err = []\n",
    "test_err = []\n",
    "\n",
    "for n_estimator in range(1,30): #[1,2,3,4,5,10,20,50]:\n",
    "    GBclassifier = GradientBoostingClassifier(n_estimators=n_estimator).fit(xtrain,ytrainClassifier)\n",
    "    yhatClassifier = GBclassifier.predict(xtrain)\n",
    "    n_estimators.append(n_estimator)\n",
    "    train_err.append(accuracy_score(ytrainClassifier, yhatClassifier))\n",
    "    test_err.append(accuracy_score(ytestClassifier, GBclassifier.predict(xtest)))\n",
    "    \n",
    "plt.plot(n_estimators, train_err, label=\"Train\")\n",
    "plt.plot(n_estimators, test_err, label=\"Test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBclassifier = GradientBoostingClassifier(n_estimators=20).fit(xtrain,ytrainClassifier)\n",
    "plt.hist(ytestClassifier, label=\"True\")\n",
    "plt.hist(GBclassifier.predict(xtest), label=\"Predicted\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ytrainClassifier, GBclassifier.predict(xtrain), label=\"Train\")\n",
    "plt.scatter(ytestClassifier, GBclassifier.predict(xtest), label=\"Test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regressor\n",
    "\n",
    "Now i will train a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBregressor = GradientBoostingRegressor().fit(xtrain,ytrain)\n",
    "yhatRegressor = GBregressor.predict(xtrain)\n",
    "yhatRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhatRegressor = GBregressor.predict(xtest)\n",
    "mean_squared_error(ytest, yhatRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = []\n",
    "train_err = []\n",
    "test_err = []\n",
    "\n",
    "for n_estimator in range(1,50): #[1,2,3,4,5,10,20,50]:\n",
    "    GBregressor = GradientBoostingRegressor(n_estimators=n_estimator).fit(xtrain,ytrain)\n",
    "    yhatRegressor = GBregressor.predict(xtrain)\n",
    "    n_estimators.append(n_estimator)\n",
    "    train_err.append(mean_squared_error(ytrain, yhatRegressor))\n",
    "    test_err.append(mean_squared_error(ytest, GBregressor.predict(xtest)))\n",
    "    \n",
    "plt.plot(n_estimators, train_err, label=\"Train\")\n",
    "plt.plot(n_estimators, test_err, label=\"Test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBregressor = GradientBoostingRegressor(n_estimators=30).fit(xtrain,ytrain)\n",
    "plt.hist(GBregressor.predict(xtest), label=\"Predicted\")\n",
    "plt.hist(ytest, label=\"True\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ytrain, GBregressor.predict(xtrain), label=\"Train\")\n",
    "plt.scatter(ytest, GBregressor.predict(xtest), label=\"Test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We finally look at 2 different Advanced Models that helps us visualize the data\n",
    "# SVR\n",
    "# MLPRegressor\n",
    "# Before we begin with these Models, we need to format the data in a manner in which we can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer and formatting data\n",
    "# Before we begin with these Models, we need to format the data in a manner in which we can work with, but this time, is a smarter manner using an Imputer to populate missing values for us\n",
    "# xmissing = dfclean[['jailprison' ,'Total number of calls',\n",
    "# 'Total duration of calls',\n",
    "# 'Total incoming calls',\n",
    "# 'Total outgoing calls',\n",
    "# 'Total number of missed calls',\n",
    "# 'Total number of incoming calls, excluding missed',\n",
    "# 'Proportion of incoming calls answered',\n",
    "# 'Proportion of calls incoming',\n",
    "# 'Duration of incoming calls',\n",
    "# 'Duration of outgoing calls',\n",
    "# 'Total number of night calls',\n",
    "# 'Any night call',\n",
    "# 'Number of incoming calls at night',\n",
    "# 'Number of outgoing calls at night',\n",
    "# 'Any night incoming call',\n",
    "# 'Any night outgoing call',\n",
    "# 'Min days between calls',\n",
    "# 'Max days between calls',\n",
    "# 'Total number of texts',\n",
    "# 'Any night text',\n",
    "# 'Number of texts at night',\n",
    "# 'Min days between texts',\n",
    "# 'Max days between texts']].values\n",
    "\n",
    "all_cols = set(dfcleanX.columns.values)\n",
    "all_cols = all_cols.difference(['Contact.ID', 'close', 'baseline_flag', 'subjectid', 'cat', 'empstatus', 'svy_contact'])\n",
    "xmissing = dfcleanX[[i for i in all_cols]].values\n",
    "\n",
    "# dfRawFilled = pandas.DataFrame(data=xdata)\n",
    "imp = Imputer()\n",
    "xdataimputed = imp.fit_transform(xmissing)\n",
    "# imp.transform(dfRawFilled.values)\n",
    "# dfRawFilled = pandas.DataFrame(data=(imp.transform(dfRawFilled.values)))\n",
    "# dfRawFilled\n",
    "xdataimputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdataimputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implement a random 80/20 split of the data to be used for train/test\n",
    "# namimg variables that will be used for training/testing\n",
    "xtrain = xdataimputed[train_indices]\n",
    "xtest = xdataimputed[test_indices]\n",
    "\n",
    "# ravel creates 1d array\n",
    "y = dfclean[['close']].values.ravel()\n",
    "ytrain = y[train_indices]\n",
    "ytest = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "\n",
    "svr = SVR(kernel='rbf', C=1e1, epsilon=0.2)\n",
    "svr.fit(xtrain, ytrain)\n",
    "yhatsvr = svr.predict(xtest)\n",
    "yhatsvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lCs = []\n",
    "train_err = []\n",
    "test_err = []\n",
    "\n",
    "for lC in range(-2,5): #[1,2,3,4,5,10,20,50]: range(1,30)\n",
    "    svr = SVR(kernel='rbf', C=pow(10,lC)).fit(xtrain,ytrain)\n",
    "    lCs.append(lC)\n",
    "    train_err.append(mean_squared_error(ytrain, svr.predict(xtrain)))\n",
    "    test_err.append(mean_squared_error(ytest, svr.predict(xtest)))\n",
    "    \n",
    "plt.plot(lCs, train_err, label=\"Train\")\n",
    "plt.plot(lCs, test_err, label=\"Test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel='rbf', C=1e2).fit(xtrain,ytrain)\n",
    "plt.hist(svr.predict(xtest), label=\"Predicted\")\n",
    "plt.hist(ytest, label=\"True\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ytrain, svr.predict(xtrain), label=\"Train\")\n",
    "plt.scatter(ytest, svr.predict(xtest), label=\"Test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPRegressor\n",
    "mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(5,2))\n",
    "mlp.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = mlp.predict(xtest)\n",
    "mean_squared_error(ytest, mlp.predict(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = []\n",
    "train_err = []\n",
    "test_err = []\n",
    "\n",
    "for h in range(5,55,3): #[1,2,3,4,5,10,20,50]: range(1,30)\n",
    "    mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(h,5))\n",
    "    mlp.fit(xtrain, ytrain)\n",
    "    hs.append(h)\n",
    "    train_err.append(mean_squared_error(ytrain, mlp.predict(xtrain)))\n",
    "    test_err.append(mean_squared_error(ytest, mlp.predict(xtest)))\n",
    "    \n",
    "plt.plot(hs, train_err, label=\"Train\")\n",
    "plt.plot(hs, test_err, label=\"Test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter out the specific columns we are interested in\n",
    "\n",
    "# xdatafull = raw_data[['jailprison' ,'Total number of calls',\n",
    "# 'Total duration of calls',\n",
    "# 'Total incoming calls',\n",
    "# 'Total outgoing calls',\n",
    "# 'Total number of missed calls',\n",
    "# 'Total number of incoming calls, excluding missed',\n",
    "# 'Proportion of incoming calls answered',\n",
    "# 'Proportion of calls incoming',\n",
    "# 'Duration of incoming calls',\n",
    "# 'Duration of outgoing calls',\n",
    "# 'Total number of night calls',\n",
    "# 'Any night call',\n",
    "# 'Number of incoming calls at night',\n",
    "# 'Number of outgoing calls at night',\n",
    "# 'Any night incoming call',\n",
    "# 'Any night outgoing call',\n",
    "# 'Min days between calls',\n",
    "# 'Max days between calls',\n",
    "# 'Total number of texts',\n",
    "# 'Any night text',\n",
    "# 'Number of texts at night',\n",
    "# 'Min days between texts',\n",
    "# 'Max days between texts']].values\n",
    "\n",
    "all_cols = set(updated_raw_data_original.columns.values)\n",
    "all_cols = all_cols.difference(['Contact.ID', 'close', 'baseline_flag', 'subjectid', 'cat', 'empstatus', 'svy_contact'])\n",
    "xdatafull = dfcleanX[[i for i in all_cols]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRawFilledFull = pandas.DataFrame(data=xdatafull)\n",
    "print(dfRawFilledFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Imputer()\n",
    "imp.fit(dfRawFilledFull)\n",
    "imp.transform(dfRawFilledFull.values)\n",
    "dfRawFilledFull = pandas.DataFrame(data=(imp.transform(dfRawFilledFull.values)))\n",
    "dfRawFilledFull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update x to use values that have been populated now\n",
    "#xdatafullColumns = dfRawFilledFull[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_cols = dfRawFilledFull.columns.size\n",
    "# num_of_cols_array = []\n",
    "# for i in range(num_of_cols):\n",
    "#     num_of_cols_array.append(i)\n",
    "    \n",
    "# print(num_of_cols_array)\n",
    "\n",
    "\n",
    "# update x to use values that have been populated now\n",
    "xdatafullColumns = dfRawFilledFull[[i for i in range(dfRawFilledFull.columns.size)]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "xdatafullColumns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBregressor = GradientBoostingRegressor().fit(xtrain,ytrain)\n",
    "yhatRegressor = GBregressor.predict(xdatafullColumns)\n",
    "\n",
    "# Do one last final predict on best results and feed out \n",
    "updated_raw_data['Predicted Closeness'] = yhatRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we write out the new dataFrame with the \"Predicted Closeness\" column\n",
    "\n",
    "dfPredictions = raw_data.to_csv('Updated_Closeness_Predictions_base%s_scale%s_missing%s.csv' % (filter_by_baseline, add_scale_cols, add_missing_01_cols), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
